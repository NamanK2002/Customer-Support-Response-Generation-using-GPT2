from nltk.translate.bleu_score import sentence_bleu

def compute_bleu_score(reference, candidate):
    """
    Compute the BLEU score for a single reference and candidate pair.

    :param reference: The reference response (ground truth).
    :param candidate: The candidate response (generated by the model).
    :return: BLEU score.
    """
    reference = [reference.split()]
    candidate = candidate.split()
    score = sentence_bleu(reference, candidate)
    return score

def print_evaluation_metrics(true_responses, generated_responses):
    """
    Print evaluation metrics for the generated responses.

    :param true_responses: List of true responses (ground truth).
    :param generated_responses: List of generated responses by the model.
    """
    bleu_scores = []

    for true_response, generated_response in zip(true_responses, generated_responses):
        score = compute_bleu_score(true_response, generated_response)
        bleu_scores.append(score)

    average_bleu_score = sum(bleu_scores) / len(bleu_scores)
    print(f'Average BLEU Score: {average_bleu_score}')
